{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX-0Lc6Q92ps"
      },
      "source": [
        "**IMDB Sentiment classification** \n",
        "**1813068 -> Nakul Chamariya**\n",
        "**1813083 -> Priyanuj Bordoloi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM5Yk-M88cpH"
      },
      "source": [
        "We start by importing the required dependencies to preprocess our data and build our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu8XC28x8adC"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k-HZKhL8hQg"
      },
      "source": [
        "Downloading the IMDB dataset, which is, fortunately, already built into Keras. Since we want to avoid a 50/50 train test split, we will immediately merge the data into data and targets after downloading so we can do an 80/20 split later on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgoFESqQ6s4I",
        "outputId": "30b791b7-deba-4f17-a790-3bc1f794840f"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYJeMCKo8tVB"
      },
      "source": [
        "exploring the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7AW6vb96s9k",
        "outputId": "236bea1b-39b3-4928-bca9-99f3526f7cab"
      },
      "source": [
        "print(\"Categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categories: [0 1]\n",
            "Number of unique words: 9998\n",
            "Average Review length: 234.75892\n",
            "Standard Deviation: 173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OckNZjv8ych"
      },
      "source": [
        "We can see in the output above that the dataset is labeled into two categories, — 0 or 1, which represents the sentiment of the review. The whole dataset contains 9,998 unique words and the average review length is 234 words, with a standard deviation of 173 words.\n",
        "\n",
        "Let's look at a single training example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT1zHcz26tDk",
        "outputId": "5794fbe2-7cd9-4b1d-da57-49e8aa69bdf5"
      },
      "source": [
        "print(\"Label:\", targets[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDZhwq6Q67Il",
        "outputId": "074a3194-f4c0-4f8c-b5a1-55854927cd36"
      },
      "source": [
        "print(data[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmt43vAZ85Zh"
      },
      "source": [
        "Above we can see the first review of the dataset, which is labeled as positive (1). The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. It replaces every unknown word with a “#”. It does this by using the get_word_index() function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxN6JR0j67LT",
        "outputId": "fbd33d1d-a545-4cd2-ff96-c0b4be3ff646"
      },
      "source": [
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
        "print(decoded) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbX4gVon9DlC"
      },
      "source": [
        "Now it's time to prepare our data. We will vectorize every review and fill it with zeros so it contains exactly 10,000 numbers. That means we fill every review that is shorter than 10,000 with zeros. We need to do this because the biggest review is nearly that long and every input for our neural network needs to have the same size. We will also transform the targets into floats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzZVzW9R67Nc"
      },
      "source": [
        "def vectorize(sequences, dimension = 10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1\n",
        "  return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjF8XNJB9GhF"
      },
      "source": [
        "Now we split our data into a training and a testing set. The training set will contain 40,000 reviews and the testing set 10,000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e81_QJ106tIM"
      },
      "source": [
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nUf7qr19MYT"
      },
      "source": [
        "Now we're ready to build our simple neural network. We'll start by defining the type of model we want to build. There are two types of models available in Keras: the sequential model and the model class used with functional API.\n",
        "\n",
        "Next we simply add the input-, hidden- and output-layers. Between them, we are using dropout to prevent overfitting. Please note you should always use a dropout rate between 20% and 50%.\n",
        "\n",
        "We use “dense” at every layer, which means the units are fully connected. Within the hidden-layers we use the relu function because this is always a good start and yields a satisfactory result most of the time. Feel free to experiment with other activation functions.\n",
        "\n",
        "At the output-layer we use the sigmoid function, which maps the values between 0 and 1. Note that we set the input-shape to 10,000 at the input-layer because our reviews are 10,000 integers long. The input-layer takes 10,000 as input and outputs it with a shape of 50.\n",
        "\n",
        "Lastly, we let Keras print a summary of the model we have just built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaAr10T-7Y6t",
        "outputId": "49f3667c-c6d0-4059-d847-b8966860cef3"
      },
      "source": [
        "model = models.Sequential()\n",
        "# Input - Layer\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "# Hidden - Layers\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "# Output- Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 505,201\n",
            "Trainable params: 505,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8hD_DEE9PlK"
      },
      "source": [
        "Now we compile our model, which is nothing but configuring the model for training. We use the “adam” optimizer, an algorithm that changes the weights and biases during training. We also choose binary-crossentropy as loss (because we deal with binary classification) and accuracy as our evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywbt4H9j7Y9K"
      },
      "source": [
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_InWiFTd9SkT"
      },
      "source": [
        "Now we're able to train our model. We'll do this with a batch_size of 500 and only for two epochs because I recognized that the model overfits if we train it longer.\n",
        "\n",
        "The batch size defines the number of samples that will be propagated through the network and an epoch is an iteration over the entire training data. In general, a larger batch size results in faster training, but doesn't always converge as fast. A smaller batch size is slower in training but it can converge faster. This is definitely problem dependent and you'll need to try out a few different values. If you start with a problem for the first time, I recommend first using a batch-size of 32, which is the standard size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaWSktEN7Y_n",
        "outputId": "6a9d0809-3b26-4477-ff01-12e5c16a63d4"
      },
      "source": [
        "results = model.fit(\n",
        " train_x, train_y,\n",
        " epochs= 50,\n",
        " batch_size = 500,\n",
        " validation_data = (test_x, test_y)\n",
        ")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "80/80 [==============================] - 3s 20ms/step - loss: 0.5472 - accuracy: 0.7190 - val_loss: 0.2631 - val_accuracy: 0.8936\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.2191 - accuracy: 0.9181 - val_loss: 0.2610 - val_accuracy: 0.8930\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.1567 - accuracy: 0.9432 - val_loss: 0.2833 - val_accuracy: 0.8916\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.1085 - accuracy: 0.9626 - val_loss: 0.3223 - val_accuracy: 0.8900\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0784 - accuracy: 0.9736 - val_loss: 0.3686 - val_accuracy: 0.8861\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0509 - accuracy: 0.9834 - val_loss: 0.4405 - val_accuracy: 0.8849\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0412 - accuracy: 0.9869 - val_loss: 0.4961 - val_accuracy: 0.8813\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0363 - accuracy: 0.9874 - val_loss: 0.5328 - val_accuracy: 0.8824\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0295 - accuracy: 0.9901 - val_loss: 0.5478 - val_accuracy: 0.8819\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0259 - accuracy: 0.9913 - val_loss: 0.5918 - val_accuracy: 0.8823\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0261 - accuracy: 0.9909 - val_loss: 0.6215 - val_accuracy: 0.8816\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0234 - accuracy: 0.9915 - val_loss: 0.6674 - val_accuracy: 0.8816\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0229 - accuracy: 0.9921 - val_loss: 0.6049 - val_accuracy: 0.8813\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0217 - accuracy: 0.9920 - val_loss: 0.6922 - val_accuracy: 0.8843\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0198 - accuracy: 0.9932 - val_loss: 0.6542 - val_accuracy: 0.8796\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0199 - accuracy: 0.9930 - val_loss: 0.6729 - val_accuracy: 0.8815\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0172 - accuracy: 0.9935 - val_loss: 0.6423 - val_accuracy: 0.8774\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0195 - accuracy: 0.9934 - val_loss: 0.6977 - val_accuracy: 0.8797\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0186 - accuracy: 0.9933 - val_loss: 0.6688 - val_accuracy: 0.8828\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0180 - accuracy: 0.9941 - val_loss: 0.7102 - val_accuracy: 0.8830\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0167 - accuracy: 0.9942 - val_loss: 0.6535 - val_accuracy: 0.8831\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0159 - accuracy: 0.9939 - val_loss: 0.7054 - val_accuracy: 0.8835\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0137 - accuracy: 0.9953 - val_loss: 0.7142 - val_accuracy: 0.8826\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0140 - accuracy: 0.9948 - val_loss: 0.7352 - val_accuracy: 0.8797\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0182 - accuracy: 0.9933 - val_loss: 0.6996 - val_accuracy: 0.8830\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 0.6843 - val_accuracy: 0.8801\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0150 - accuracy: 0.9945 - val_loss: 0.7124 - val_accuracy: 0.8817\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.7128 - val_accuracy: 0.8793\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0118 - accuracy: 0.9957 - val_loss: 0.7666 - val_accuracy: 0.8784\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.7850 - val_accuracy: 0.8801\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0117 - accuracy: 0.9958 - val_loss: 0.7585 - val_accuracy: 0.8824\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0129 - accuracy: 0.9952 - val_loss: 0.7459 - val_accuracy: 0.8800\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0124 - accuracy: 0.9953 - val_loss: 0.7695 - val_accuracy: 0.8831\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0126 - accuracy: 0.9949 - val_loss: 0.7901 - val_accuracy: 0.8821\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0120 - accuracy: 0.9956 - val_loss: 0.8046 - val_accuracy: 0.8808\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0114 - accuracy: 0.9956 - val_loss: 0.7966 - val_accuracy: 0.8816\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.8216 - val_accuracy: 0.8830\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0141 - accuracy: 0.9946 - val_loss: 0.8103 - val_accuracy: 0.8802\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0132 - accuracy: 0.9949 - val_loss: 0.7843 - val_accuracy: 0.8799\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.8206 - val_accuracy: 0.8825\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0106 - accuracy: 0.9963 - val_loss: 0.8231 - val_accuracy: 0.8827\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0103 - accuracy: 0.9964 - val_loss: 0.8222 - val_accuracy: 0.8792\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0118 - accuracy: 0.9958 - val_loss: 0.8237 - val_accuracy: 0.8817\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.8754 - val_accuracy: 0.8838\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0090 - accuracy: 0.9962 - val_loss: 0.8869 - val_accuracy: 0.8812\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0111 - accuracy: 0.9958 - val_loss: 0.8245 - val_accuracy: 0.8803\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0089 - accuracy: 0.9964 - val_loss: 0.8517 - val_accuracy: 0.8822\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 0.8371 - val_accuracy: 0.8811\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.8332 - val_accuracy: 0.8820\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0108 - accuracy: 0.9957 - val_loss: 0.8497 - val_accuracy: 0.8843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrCwxyYm7pyK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozgw2yjL7p07"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}